Here is a **clean, professional, concise README.md** for your project â€” not too long, but complete enough for GitHub and recruiters.

---

# ğŸ“˜ **Customer Credit Risk & Behaviour Scoring**

A full end-to-end machine learning system for **credit risk assessment**, including:

* **PD (Probability of Default)**
* **LGD (Loss Given Default)**
* **Expected Loss = PD Ã— LGD**
* **Customer Segmentation (KMeans)**
* **Behavioural Anomaly Detection (Isolation Forest)**
* **Time-Series Default Rate Forecasting (ARIMA)**
* **Autoencoder for deep behavioural embeddings**
* **FastAPI model-serving microservice**
* **Streamlit dashboard for interactive scoring**

Built using Python, DuckDB, XGBoost, LightGBM, PyTorch, and FastAPI.

---

## ğŸš€ Features

### ğŸ§  Machine Learning Models

* **PD Model:** XGBoost classifier using 10 engineered financial features
* **LGD Model:** LightGBM regressor with realistic synthetic LGD target
* **Expected Loss:** Automatically computed inside API
* **Segmentation:** KMeans clustering of scaled financial profiles
* **Anomaly Detection:** Isolation Forest on customer behaviour
* **Autoencoder:** 8-dim latent embeddings (PyTorch)
* **Time-Series:** ARIMA model for monthly default rate

### ğŸ”§ Engineering & Serving

* **FastAPI** microservice for scoring (`/predict`)
* **Dockerfile** for containerized deployment
* **Model loader with caching** for fast inference
* **Consistent feature ordering** via `feature_cols.json`

### ğŸ“Š Dashboard

Interactive **Streamlit dashboard** with:

* Human-friendly input fields
* Tooltips describing each feature
* Clean, fintech-style UX
* Displays PD, Expected Loss, Cluster, and Anomaly Score

---

## ğŸ“‚ Project Structure

```
project/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ features_sql.py        # DuckDB-based feature engineering
â”‚   â”œâ”€â”€ train_pd_model.py      # PD model training
â”‚   â”œâ”€â”€ train_lgd_model.py     # LGD model training
â”‚   â”œâ”€â”€ train_unsupervised.py  # KMeans + IsolationForest
â”‚   â”œâ”€â”€ train_timeseries.py    # ARIMA default rate forecast
â”‚   â”œâ”€â”€ train_autoencoder.py   # PyTorch autoencoder
â”‚   â”œâ”€â”€ config.py              # Paths
â”‚   â””â”€â”€ utils.py               # Shared utilities
â”‚
â”œâ”€â”€ service/
â”‚   â”œâ”€â”€ app.py                 # FastAPI app
â”‚   â”œâ”€â”€ model_loader.py        # Cached model loading + scaling
â”‚   â””â”€â”€ schemas.py             # Input/output schemas
â”‚
â”œâ”€â”€ monitoring/
â”‚   â””â”€â”€ dashboard_streamlit.py # Streamlit frontend
â”‚
â”œâ”€â”€ models/                    # Saved models & scalers
â”œâ”€â”€ data/                      # Raw + processed data
â””â”€â”€ Dockerfile                 # FastAPI deployment
```

---

## â–¶ï¸ Running the System

### **1. Start FastAPI**

```bash
uvicorn service.app:app --reload --host 0.0.0.0 --port 8000
```

### **2. Start Streamlit**

```bash
streamlit run monitoring/dashboard_streamlit.py
```

### API Docs

[http://localhost:8000/docs](http://localhost:8000/docs)

### Streamlit Dashboard

[http://localhost:8501](http://localhost:8501)

---

## ğŸ“ˆ Example Predictions

### Good customer:

* PD â‰ˆ **0.05**
* Expected Loss â‰ˆ **400**
* Cluster = **3**
* Anomaly Score = small/negative

### Risky customer:

* PD â‰ˆ **0.8**
* Expected Loss â‰ˆ **13k**
* Cluster = **3**
* Anomaly Score = positive

---

## ğŸ›  Technologies Used

* Python 3.11
* DuckDB
* XGBoost, LightGBM
* PyTorch
* FastAPI
* Streamlit
* scikit-learn
* statsmodels (ARIMA)
* Docker

---

## ğŸ“„ License

MIT License â€” free to use, modify, and build upon.

---